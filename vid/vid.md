---

title: 视频目标检测

---

对于视频来讲，相邻帧目标之间存在明显的上下文关系，这种关系在技术上的表现就是Tracking。经典的单目标跟踪算法TLD,通过Tracking-Learning-Detection学习目标的帧间变换，并进行Location。
从2015年ImageNet在目标检测下新增了VID挑战，也是将目标检测带到视频领域成为热点问题的开端。主要思路是结合帧间信息，跟踪信息。其实这里看起来很熟悉，因为有一个很古老的话题，就是运动目标检测的方法就是利用帧差、光流以及图像帧的空域信息。有一些有代表性的视频目标检测方法。
T-CNN的方法结合静态检测信息、上下文信息和跟踪信息，首先通过静态的图片检测出结果，然后用上下文抑制降低监测置信度/分数（score）比较低的部分，利用运动传播（光流信息）降低漏检率，接着结合跟踪信息进行tubelet（管道滤波），用跟踪的值修正检测，最终利用建议框合并或非极大值抑制的方法消除重复框。
Deep Feature Flow结合光流的思路，实现特征图的帧间传播和复用。算法首先在关键帧（Key Frame）进行特诊图提取，然后将特征在帧间传播，最后进行特征图映射，然后进行检测。
Seq-NMS在视频连续帧中沿着高置信度的bounding boxes构建一个序列，和序列相邻的框被抑制。
MCMOT把检测的后处理问题当做了多目标跟踪问题，尝试了许多诸如颜色、置信度，光流等等的手工特征得到后验概率最大值，然后对检测框进行筛选。
FGFA这个是Deep feature flow的后续工作，ImageNet2017年的冠军。算法的亮点在于对视频中运动模糊的目标检测效果非常好，首先用特征网络从图片中提空间特征，然后用光流网络计算出帧间的光流信息，与第一部分的特征融合得到特征图，然后这样得到每一帧视频的特征图，然后对当前特征图的前后K帧进行加权的特征融合，（这个权值根据结果的置信更新），得到融合特征后进行检测，后接的是目标检测网络，得到结果。整个网络训练是端到端的，降低了模块之间的误差。
